{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIAP Housing Price Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Machine Learning Pipeline\n",
    "this version does not use sklearn pipeline()\n",
    "\n",
    "* Ingest data\n",
    "* data cleanse/prep\n",
    "* Select Algo\n",
    "* Tune\n",
    "* Eval\n",
    "\n",
    "Also,\n",
    "\n",
    "* use Fns\n",
    "* output relevant training/eval metrics \n",
    "* Create config files for easy experimentation of different algo and parameters\n",
    "* create bash script run.sh at base folder\n",
    "* create requirements.txt \n",
    "* create readme.md that explains program design and usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical libs and data handing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# QuantitleTransformer can force any arbitrary distribution into a gaussian, \n",
    "# provided that there are enough training samples (thousands). \n",
    "# Because it is a non-parametric method, it is harder to interpret than \n",
    "# the parametric ones (Box-Cox and Yeo-Johnson).\n",
    "\n",
    "\n",
    "import geohash2 # for latitude/longtitude feature engineering\n",
    "# https://anaconda.org/conda-forge/geohash2\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ML Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "#import sklearn.linear_model as linear_model\n",
    "# neural networks?\n",
    "# K-Nearest Neighbors vs KMeans\n",
    "# random forest?\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluate Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ipython display\n",
    "%matplotlib inline\n",
    "#pd.options.display.max_rows = 1000\n",
    "#pd.options.display.max_columns = 20\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of cross validations used in the Model part \n",
    "cvNum = 5\n",
    "\n",
    "# on/off switch for using log values for House Price and features     \n",
    "use_logvals = 1    \n",
    "\n",
    "# target used for correlation \n",
    "target = 'Y house price of unit area'\n",
    "\n",
    "# attributes shortcuts\n",
    "x1 = 'X1 transaction date'\n",
    "x2 = 'X2 house age'\n",
    "x3 = 'X3 distance to the nearest MRT station'\n",
    "x4 = 'X4 number of convenience stores'\n",
    "x5 = 'X5 latitude'\n",
    "x6 = 'X6 longitude'\n",
    "\n",
    "# only columns with correlation above this threshold value are used for the ML Regressors \n",
    "corr_Floor = 0.4    \n",
    "    \n",
    "# on/off switch for dropping columns that are similar to others already used and show a high correlation to these     \n",
    "drop_similar = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Load Data, transform and split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://aisgaiap.blob.core.windows.net/aiap4-assessment/real_estate.csv')\n",
    "\n",
    "# load actions to load into pipeline()\n",
    "# drop id, drop others?\n",
    "# transform g\n",
    "### do the geohash\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([target],axis=1), df[target], test_size=0.33, random_state=88)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Data Preprocessing\n",
    "Feature Engineering and Selection\n",
    "\n",
    "To prep data for ingestion into ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.1 Drop Unique Identifer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'No' in X_train:\n",
    "    X_train.drop(['No'],axis=1,inplace=True)\n",
    "\n",
    "if 'No' in X_test:\n",
    "    X_test.drop(['No'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Drop Constant or Quasi-Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features that are constant or quasi-constant have no or very little informational value to the ML model.\n",
      "Rather than suffer additional computational complexity, it is better to drop those attributes.\n",
      "\n",
      "Constant Features is \n",
      "[]\n",
      "\n",
      "There are no constant features\n"
     ]
    }
   ],
   "source": [
    "print('Features that are constant or quasi-constant have no or very little informational value to the ML model.')\n",
    "print('Rather than suffer additional computational complexity, it is better to drop those attributes.')\n",
    "\n",
    "features_const = [ feat for feat in X_test.columns if X_test[feat].std() ==0]\n",
    "\n",
    "print('\\nConstant Features is \\n{}'.format(features_const))\n",
    "\n",
    "print('\\nThere are no constant features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Select Features based on correlation to Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index(['X3 distance to the nearest MRT station', 'X4 number of convenience stores'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.drop([x1,x2,x5,x6],axis=1)\n",
    "print()\n",
    "print(X_test.columns)\n",
    "print()\n",
    "\n",
    "X_train = X_train.drop([x1,x2,x5,x6],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Distribution of Target and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, this is not used.\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html\n",
    "# On “small” datasets (less than a few hundred points), the quantile transformer is prone to overfitting. \n",
    "# The use of the power transform is then recommended.\n",
    "\n",
    "#bc = PowerTransformer(method='box-cox') # can only be used for +ve values\n",
    "#X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "\n",
    "#yj = PowerTransformer(method='yeo-johnson') \n",
    "#X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "# error: Input contains infinity or a value too large for dtype('float64').\n",
    "\n",
    "#qt = QuantileTransformer(output_distribution='normal', random_state=88)\n",
    "#X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "#type(X_trans_qt)\n",
    "#dfa = pd.DataFrame(X_trans_qt,columns=[x2,x3])\n",
    "\n",
    "print('Currently, this is not used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I have only two attributes, it is not necessary to do dimensionality reduction using algorithms such as PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Select Machine Learn Algorithm and GridSearch Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose ElasticNet as default\n",
    "\n",
    "# if others, load from config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Load Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('elasticnet', ElasticNet())]\n",
    "         \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet Alpha: {'elasticnet__l1_ratio': 1.0}\n",
      "Tuned ElasticNet R squared: 0.481090617451612\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline, parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Predict and Evaluate Model -ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = gm_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.481090617451612\n",
      "Root Mean Squared Error: 8.621735231318153\n"
     ]
    }
   ],
   "source": [
    "# Compute and print R^2 and RMSE\n",
    "\n",
    "print(\"R^2: {}\".format(gm_cv.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.48025659735708476\n",
      "Root Mean Squared Error: 8.628661116328413\n"
     ]
    }
   ],
   "source": [
    "# Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'alpha': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n",
      "0.5074324734475906\n",
      "\n",
      "1.0\n",
      "\n",
      "\n",
      "R^2: 0.48025659735708476\n",
      "Root Mean Squared Error: 8.217553206553424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "\n",
    "grid.fit(X_test, y_test)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print()\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "print('\\n')\n",
    "y_pred = grid.predict(X_test)\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
